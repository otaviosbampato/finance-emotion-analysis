# -*- coding: utf-8 -*-
"""finance-emotions-fine-tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-3-tEVu2CxrVYz0qsFaDKQPaReXkUpnu

## Authorship
OtÃ¡vio Sbampato
> Hugging Face - https://huggingface.co/otaviosbampato  
> GitHub - https://github.com/otaviosbampato

## Preparation

- we need to firstly change our notebook configuration to use a GPU.  
- then, we will install our pre-requisites.
"""

!pip install -U transformers datasets evaluate
# engine, datasets

import transformers; print(transformers.__version__)

"""## Preparing our dataset

url: https://huggingface.co/datasets/vamossyd/finance_emotions
"""

from datasets import load_dataset

dataset_name = "vamossyd/finance_emotions"
train_set = load_dataset(dataset_name, split="train[:90%]")
test_set = load_dataset(dataset_name, split="train[90%:]")

train_set

train_set[65]

"""## Tokenizer

we'll now transform words and texts of interest into numbers, tokenizing them.
"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
# this tokenizer takes care of casing too.

# train_set[100:105]
label_to_value = {
    "neutral": 0,
    "happy": 1,
    "fear": 2,
    "surprise": 3,
    "sad": 4,
    "disgust": 5,
    "anger": 6
}

def tokenizer_func(batch):
  tokenized_batch = tokenizer(
      batch['cleaned_text'],
      padding="max_length",
      truncation=True)

  # for each label in our batch of data we pass as a param to the function,
  # we run it trough our set to get its label number,
  # then we replace all values of 'label' in tokenized_batch for its number.
  tokenized_batch['label'] = [label_to_value[label] for label in batch['label']]

  # then we return it.
  return tokenized_batch

"""> - above, we tokenize the cleaned_text part of our batch of data, pad the text to max_length (for bert, 512 words), and set truncation to true.
> - in case a cleaned_text had less than 512 words, the rest of the words would be filled as 0's, which the tokenizer would ignore.


> > *   for strings such small as ours, a most likely better approach would be to use padding=True, which would pad the maximum length at the biggest word count string on each batch.
> > *   in a study-case scenario such as this, however, its interesting to keep it in a max_length, which keeps stability (although adds much much noise).


> - in case it had MORE than 512 words, the tokenizer would just truncate it.

**wait, what even is a tokenizer?**

a computer is not so great at recognizing words as we are. (or, even if it were, doing so would be much much expensive in computation). <br/>
therefore we have a tokenizer! the purpose of a tokenizer is to map those words into numbers. that makes all our lives much easier. <br/>
check this example below to understand more thoroughly.

suppose we have 2 phrases:

1.   I enjoy driving my car  
2.   I really enjoy reading good books
  
mapping each unique word out to a number becomes:  

I -> 0  
enjoy -> 1  
driving -> 2  
my -> 3  
car -> 4  
really -> 5  
reading -> 6  
good -> 7  
books -> 8  
  
*tokenizer comes in*  
phrase 1 becomes: 0 1 2 3 4 5  
phrase 2 becomes: 0 5 1 6 7 8  
and so on.  
  
this makes it much easier for the model to interpret text and do its magic.  

> it is worth noting that some tokenizers differ in behavior. some may do subwording (*driving -> 2*  would then become *driv -> 2* and *ing -> 3* ), which may help performance, and some might even map out every letter and construct text based on that.
"""

tokenized_train_data = train_set.map(tokenizer_func, batched=True)
tokenized_test_data = test_set.map(tokenizer_func, batched=True)

"""## Importing the model"""

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=7)

"""here we use BERT, an encoder-only transformer model; this is particularly good given the intricate encoder-only transformers qualities on context comprehension and sentiment analysis.  
> encoder-only transformers do NOT generate new text (differs from GPT models). for our use case, they are the correct choice.
"""

from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="finance-model")

!pip install evaluate

import numpy as np
import evaluate

"""these two libs will help us evaluate accuracy and other metrics easier."""

metric = evaluate.load("accuracy")

def compute_metrics(evaluate_prediction):
  logits, labels = evaluate_prediction
  predictions = np.argmax(logits, axis=-1)
  return metric.compute(predictions=predictions, references=labels)

"""we'll implement training now."""

training_args = TrainingArguments(output_dir="finance-model",
                                  eval_strategy="epoch",
                                  report_to="none")

from transformers import Trainer

"""this trainer puts it everything together. better practice would be to separate variable naming, but for this learning environment, we keep as much variable similarity as possible on params.  
everything we've done so far leads to this very moment.
"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_data,
    eval_dataset=tokenized_test_data,
    compute_metrics=compute_metrics,
)

"""training takes ~45min. let it do its magic :)"""

trainer.train()

"""we now upload our project to huggingface!"""

!huggingface-cli login

model.save_pretrained("finance-emotions-model",
                      push_to_hub=True,
                      private=False)